{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules Imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "####### Module to retrieve pickled data######\n",
    "from sklearn.utils import shuffle\n",
    "from preprocess import load_data\n",
    "########## Plotting Data ########\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import trange\n",
    "import time\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "####### Mathematical & Array Operations #########\n",
    "import numpy as np\n",
    "import math\n",
    "from random import randint\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Tensor FLow for Neural Network Frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.contrib.layers import apply_regularization\n",
    "from tensorflow.contrib.layers import l1_regularizer\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "from spatial_transformer import transformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "########## Graph Inputs #########################\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int64, (None))\n",
    "num_classes = 43\n",
    "reg_fac = tf.placeholder(tf.float32, None)\n",
    "rate = tf.placeholder(tf.float32, None)\n",
    "is_training = tf.placeholder(tf.bool, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Batch Normalization Layer ##################\n",
    "def batch_norm(input_, name, n_out, phase_train):\n",
    "    with tf.variable_scope(name + 'bn'):\n",
    "        beta = tf.Variable(tf.constant(\n",
    "            0.0, shape=[n_out]), name=name + 'beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(\n",
    "            1.0, shape=[n_out]), name=name + 'gamma', trainable=True)\n",
    "        if len(input_.get_shape().as_list()) > 3:\n",
    "            batch_mean, batch_var = tf.nn.moments(\n",
    "                input_, [0, 1, 2], name=name + 'moments')\n",
    "        else:\n",
    "            batch_mean, batch_var = tf.nn.moments(\n",
    "                input_, [0, 1], name=name + 'moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (\n",
    "            ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(\n",
    "            input_, mean, var, beta, gamma, 1e-3)\n",
    "\n",
    "    variable_summaries(beta)\n",
    "    variable_summaries(gamma)\n",
    "    return normed\n",
    "\n",
    "############################## Parametric ReLU Activation  Layer #########\n",
    "\n",
    "\n",
    "def parametric_relu(input_, name):\n",
    "    alpha = tf.get_variable(name=name + '_alpha', shape=input_.get_shape(\n",
    "    )[-1], initializer=tf.random_uniform_initializer(minval=0.1, maxval=0.3), dtype=tf.float32)\n",
    "    pos = tf.nn.relu(input_)\n",
    "    tf.summary.histogram(name, pos)\n",
    "    neg = alpha * (input_ - abs(input_)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "\n",
    "# Convolutional Layer with activation and batc\n",
    "def conv(input_, name, k1, k2, n_o, reg_fac, is_tr, s1=1, s2=1, is_act=True, is_bn=True, padding='SAME'):\n",
    "\n",
    "    n_i = input_.get_shape()[-1].value\n",
    "    with tf.variable_scope(name):\n",
    "        weights = tf.get_variable(name + \"weights\", [k1, k2, n_i, n_o], tf.float32, xavier_initializer(\n",
    "        ), regularizer=tf.contrib.layers.l2_regularizer(reg_fac))\n",
    "        biases = tf.get_variable(name +\n",
    "                                 \"bias\", [n_o], tf.float32, tf.constant_initializer(0.0))\n",
    "        conv = tf.nn.conv2d(input_, weights, (1, s1, s2, 1), padding=padding)\n",
    "        bn = batch_norm(conv, name, n_o, is_tr) if is_bn else conv\n",
    "        activation = parametric_relu(tf.nn.bias_add(\n",
    "            bn, biases), name + \"activation\") if is_act else tf.nn.bias_add(bn, biases)\n",
    "        variable_summaries(weights)\n",
    "        variable_summaries(biases)\n",
    "    return activation\n",
    "\n",
    "# Fully connected Layer with activation and ba\n",
    "\n",
    "\n",
    "def fc(input_, name, n_o, reg_fac, is_tr, p_fc, is_act=True, is_bn=True):\n",
    "    n_i = input_.get_shape()[-1].value\n",
    "    with tf.variable_scope(name):\n",
    "        weights = tf.get_variable(name + \"weights\", [n_i, n_o], tf.float32, xavier_initializer(\n",
    "        ),  regularizer=tf.contrib.layers.l2_regularizer(reg_fac))\n",
    "        biases = tf.get_variable(\n",
    "            name + \"bias\", [n_o], tf.float32, tf.constant_initializer(0.0))\n",
    "        bn = tf.nn.bias_add(tf.matmul(input_, weights), biases)\n",
    "        activation = batch_norm(bn, name, n_o, is_tr) if is_bn else bn\n",
    "        logits = parametric_relu(\n",
    "            activation, name + \"activation\") if is_act else activation\n",
    "        \n",
    "        variable_summaries(weights)\n",
    "        variable_summaries(biases)\n",
    "\n",
    "    return tf.cond(is_tr, lambda: tf.nn.dropout(logits, keep_prob=p_fc), lambda: logits)\n",
    "\n",
    "############################# Max Pooling Layer with activation ##########\n",
    "\n",
    "\n",
    "def pool(input_, name, k1, k2, s1=2, s2=2):\n",
    "    return tf.nn.max_pool(input_, ksize=[1, k1, k2, 1], strides=[1, s1, s2, 1], padding='VALID', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Localization Layer for Spatial Transformer L\n",
    "def localization_net(input_, name, is_tr, reg_fac):\n",
    "    # Identity transformation\n",
    "    initial = np.array([[1., 0, 0], [0, 1., 0]])\n",
    "    initial = initial.astype('float32')\n",
    "    initial = initial.flatten()\n",
    "    # Weight and Bias containing the identity transformation\n",
    "    W = tf.get_variable('loc_weights', [64, 6], tf.float32, xavier_initializer(\n",
    "    ), regularizer=tf.contrib.layers.l2_regularizer(reg_fac))\n",
    "    b = tf.Variable(initial_value=initial, name='loc_bias')\n",
    "\n",
    "    ############ Localization Network for the Spatial transformer network ####\n",
    "    ##################### 7x7x16 Conv -> 2x2 Max pooling ###################\n",
    "    locnet = conv(input_, name=\"locnet_conv1\", k1=3, k2=3, n_o=16,\n",
    "                  reg_fac=reg_fac, is_tr=is_tr, padding='SAME')\n",
    "    locnet = pool(locnet, name=\"locnet_pool1\", k1=2, k2=2)\n",
    "\n",
    "    ###################### 5x5x32 Conv -> 2x2 Max pooling ###################\n",
    "    locnet = conv(locnet, name=\"locnet_conv2\", k1=3, k2=3, n_o=32,\n",
    "                  reg_fac=reg_fac, is_tr=is_tr, padding='SAME')\n",
    "    locnet = pool(locnet, name=\"locnet_pool2\", k1=2, k2=2)\n",
    "\n",
    "    ###################### 3x3x64 Conv -> 2x2 Max pooling ####################\n",
    "    locnet = conv(locnet, name=\"locnet_conv3\", k1=3, k2=3, n_o=64,\n",
    "                  reg_fac=reg_fac, is_tr=is_tr, padding='SAME')\n",
    "    locnet = pool(locnet, name=\"locnet_pool3\", k1=2, k2=2)\n",
    "    \n",
    "    ###################### 3x3x64 Conv -> 2x2 Max pooling ####################\n",
    "    locnet = conv(locnet, name=\"locnet_conv4\", k1=3, k2=3, n_o=128,\n",
    "                  reg_fac=reg_fac, is_tr=is_tr, padding='SAME')\n",
    "    locnet = pool(locnet, name=\"locnet_pool4\", k1=2, k2=2)\n",
    "\n",
    "    ####################### Fully Connected Layers ###########################\n",
    "    locnet_fc0 = flatten(locnet)\n",
    "    locnet_fc1 = fc(locnet_fc0, name=\"locnet_fc1\", n_o=128,\n",
    "                    reg_fac=reg_fac, is_tr=is_tr, p_fc=.50)\n",
    "    locnet_fc2 = fc(locnet_fc1, name=\"locnet_fc2\", n_o=64,\n",
    "                    reg_fac=reg_fac, is_tr=is_tr, p_fc=.50)\n",
    "    locnet_op = tf.nn.bias_add(tf.matmul(locnet_fc2, W), b)\n",
    "\n",
    "    return locnet_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_Layer(input_, name, conv_size, n_layers, pool_size, n_o, reg_fac, is_tr, p_vgg):\n",
    "\n",
    "    n_i = input_.get_shape()[-1].value\n",
    "    c_k1 = conv_size\n",
    "    c_k2 = conv_size\n",
    "    p_k1 = pool_size\n",
    "    p_k2 = pool_size\n",
    "\n",
    "    vgg = input_\n",
    "    for i in range(n_layers):\n",
    "        ############## VGG Building Block ###############################\n",
    "        ############## 2 - Conv , 1- Pool 1- Dropout 1 - Batch-Norm ###########\n",
    "        vgg = conv(vgg, name=name + \"conv1_\" + str(i), k1=c_k1,\n",
    "                   k2=c_k2, n_o=n_o, reg_fac=reg_fac, is_tr=is_tr)\n",
    "\n",
    "    vgg = pool(vgg, name=name + \"pool1\", k1=p_k1, k2=p_k2)\n",
    "    vgg = tf.cond(is_tr, lambda: tf.nn.dropout(\n",
    "        vgg, keep_prob=p_vgg), lambda: vgg)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input_, num_classes, params, reg_fac, is_training):\n",
    "\n",
    "    ############## Spatial Transformer Module ######################\n",
    "    ##################### Localization layer #######################\n",
    "    locnet = localization_net(input_, name=\"locnet\",\n",
    "                              reg_fac=reg_fac, is_tr=is_training)\n",
    "    ####################### Affine Transformation Layer ############\n",
    "    stn = transformer(input_, locnet, out_size=(32, 32, 3))\n",
    "    stn_= tf.reshape(stn,(-1,32,32,3))\n",
    "    ################################## VGG Net ###############################\n",
    "    ################################# VGG- Layer - 1 #########################\n",
    "    vgg1 = VGG_Layer(stn_, \"vgg1\", conv_size=3, n_layers=2, pool_size=2, n_o=32,\n",
    "                     reg_fac=reg_fac, is_tr=is_training, p_vgg=params.vgg1)\n",
    "    variable_summaries(vgg1)\n",
    "    ################################# VGG- Layer - 2 #########################\n",
    "    vgg2 = VGG_Layer(vgg1, \"vgg2\", conv_size=3,  n_layers=2, pool_size=2, n_o=64,\n",
    "                     reg_fac=reg_fac, is_tr=is_training, p_vgg=params.vgg2)\n",
    "    variable_summaries(vgg2)\n",
    "    ################################# VGG- Layer - 3 #########################\n",
    "    vgg3 = VGG_Layer(vgg2, \"vgg3\", conv_size=3,  n_layers=3, pool_size=2, n_o=128,\n",
    "                     reg_fac=reg_fac, is_tr=is_training, p_vgg=params.vgg3)\n",
    "    variable_summaries(vgg3)\n",
    "    ################################# VGG- Layer - 4 #########################\n",
    "    vgg4 = VGG_Layer(vgg3, \"vgg4\", conv_size=3,  n_layers=3, pool_size=2, n_o=256,\n",
    "                     reg_fac=reg_fac, is_tr=is_training, p_vgg=params.vgg4)\n",
    "    variable_summaries(vgg4)\n",
    "    ################################ Multi Scale Convolution layer ###########\n",
    "    ms1 = pool(vgg2, \"ms1\", k1=2, k2=2)\n",
    "    ms2 = tf.concat([ms1,vgg3], axis=3)\n",
    "    \n",
    "    ms3 = pool(ms2, \"ms3\", k1=2, k2=2)\n",
    "    ms = tf.concat([ms3, vgg4], axis=3)\n",
    "\n",
    "    \n",
    "    ################## Fully Connected Layers for a Linear Classifier ########\n",
    "    #############################First Fully Connected Layer #################\n",
    "    fc0 = fc(flatten(ms), \"fc0\", 1024, reg_fac=reg_fac,\n",
    "             is_tr=is_training, p_fc=params.fc0)\n",
    "    fc1 = fc(fc0, \"fc1\", 512, reg_fac=reg_fac,\n",
    "             is_tr=is_training, p_fc=params.fc1)\n",
    "    ########################### Output readout Layer #########################\n",
    "    fc2 = fc(fc1, \"fc2\", num_classes, reg_fac=reg_fac,\n",
    "             is_tr=is_training, p_fc=1.0, is_act=False, is_bn=True)\n",
    "\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = namedtuple('params', 'vgg1 vgg2 vgg3 vgg4 ms fc0 fc1')\n",
    "mdltype = params(vgg1=.5, vgg2=.5, vgg3=.5, vgg4=.5, ms=.5, fc0=.5, fc1=.5)\n",
    "logits = run_model(x, num_classes, mdltype, reg_fac, is_training)\n",
    "tf.summary.histogram('Logits', logits)\n",
    "\n",
    "# For Top 5 Guesses\n",
    "prediction = tf.nn.softmax(logits)\n",
    "top5_guesses = tf.nn.top_k(prediction, k=5, sorted=True)\n",
    "\n",
    "\n",
    "# Predicted Label and Actual Label using Argmax\n",
    "y_pred = tf.argmax(logits, 1)\n",
    "\n",
    "# Accuracy Calculation\n",
    "correct_prediction = tf.equal(y_pred, y)\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('Accuracy', accuracy_operation)\n",
    "\n",
    "######### Cross Entropy and Loss for Training ##########\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=y, name='cross_entropy')\n",
    "loss_operation = tf.reduce_mean(\n",
    "    cross_entropy) + tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "tf.summary.scalar('Loss', loss_operation)\n",
    "\n",
    "########### Training Step ################\n",
    "Train_Step = tf.train.AdamOptimizer(rate).minimize(loss_operation)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, batch_size, is_tr):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        bx, by = X_data[offset:offset +\n",
    "                        batch_size], y_data[offset:offset + batch_size]\n",
    "        inputs_ = {x: bx, y: by, is_training: is_tr}\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict=inputs_)\n",
    "        total_accuracy += (accuracy * len(bx))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "def plot_confusion_matrix(X_data,y_data, batch_size, is_tr, normalize=True):\n",
    "    num_examples = len(X_data)\n",
    "    sess = tf.get_default_session()\n",
    "    offset=randint(0,num_examples-batch_size)\n",
    "    bx, by = X_data[offset:offset +\n",
    "                    batch_size], y_data[offset:offset + batch_size]\n",
    "    inputs_ = {x: bx, y: by, is_training: is_tr}\n",
    "    cm_input = sess.run(y_pred,feed_dict=inputs_)\n",
    "    cm = confusion_matrix(y_true = by,y_pred = cm_input)\n",
    "    if normalize is True:\n",
    "        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis],2)\n",
    "    \n",
    "    plt.figure(figsize=(25,25))  \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    \n",
    "    df=pd.read_csv(\"signnames.csv\")\n",
    "    plt.xticks(range(num_classes), df['SignName'])\n",
    "    plt.yticks(range(num_classes), df['SignName'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xticks(fontsize = 16)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.xlabel('Predicted', fontsize = 24)\n",
    "    plt.ylabel('True', fontsize= 24)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and Modules loaded\n",
      "Data and Modules loaded\n",
      "Data and Modules loaded\n"
     ]
    }
   ],
   "source": [
    "# training_file = 'train_processed'\n",
    "# validation_file = 'valid_processed'\n",
    "# testing_file = 'test_processed'\n",
    "training_file = 'train.pickle'\n",
    "validation_file = 'valid.pickle'\n",
    "testing_file = 'test.pickle'\n",
    "X_train,y_train = load_data(training_file)\n",
    "X_valid,y_valid = load_data(validation_file)\n",
    "X_test,y_test = load_data(testing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGNet_180518\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "EPOCHS = 50\n",
    "REG_FACTOR = 2e-5\n",
    "RATE = 1e-4\n",
    "now_datetime = datetime.now().strftime(\"%y%m%d\")\n",
    "save_file = \"VGGNet_\" + now_datetime\n",
    "restore_file =\"VGGNet_\" + now_datetime\n",
    "chkpt = './' + save_file\n",
    "chkpt_restore = './' + restore_file\n",
    "logdir = chkpt + datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\n",
    "is_restore= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:19<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1\n",
      "Training_Loss: 3.6959225197263708\n",
      "Validation_Accuracy: 0.05124716644857476\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:48<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2\n",
      "Training_Loss: 3.5598613598137128\n",
      "Validation_Accuracy: 0.0637188220381855\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:55<00:00,  9.36s/it]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3\n",
      "Training_Loss: 3.527893137029507\n",
      "Validation_Accuracy: 0.04648526098849484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:40<00:00,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4\n",
      "Training_Loss: 3.4785314421499045\n",
      "Validation_Accuracy: 0.06689342399826802\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:32<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5\n",
      "Training_Loss: 3.354271733365856\n",
      "Validation_Accuracy: 0.08503401360544217\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:16<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6\n",
      "Training_Loss: 3.268015955278197\n",
      "Validation_Accuracy: 0.10317460239744511\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [09:32<00:00,  8.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7\n",
      "Training_Loss: 3.21765446485454\n",
      "Validation_Accuracy: 0.10929705326755841\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [09:19<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8\n",
      "Training_Loss: 3.1638305756500316\n",
      "Validation_Accuracy: 0.1149659828439169\n",
      "Intermediate Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/70 [01:35<07:42,  7.97s/it]"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(init)\n",
    "    ##################  Start Model Training  #######################\n",
    "    if is_restore:\n",
    "        saver.restore(sess,chkpt_restore)\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    val_acc = []\n",
    "    for i in range(EPOCHS):\n",
    "        \n",
    "        ############ Training Operation ################\n",
    "        Training_loss = 0\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in trange(0, len(X_train), BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            inputs = {x: batch_x, y: batch_y, reg_fac: REG_FACTOR,\n",
    "                      rate: RATE, is_training: True}\n",
    "            loss, _ = sess.run([loss_operation, Train_Step], feed_dict=inputs)\n",
    "            Training_loss += (loss * len(batch_x))\n",
    "\n",
    "        ################ Evaluation operation ####################\n",
    "        Validation_Accuracy = evaluate(X_valid, y_valid, 1000, is_tr=False)\n",
    "        val_acc.append(Validation_Accuracy)\n",
    "        Training_loss /= len(X_train)\n",
    "        print(\"Epochs:\", i + 1)\n",
    "        print(\"Training_Loss:\", Training_loss)\n",
    "        print(\"Validation_Accuracy:\", Validation_Accuracy)\n",
    "        \n",
    "        \n",
    "        ##### Save Model if the Validation accuracy gets better\n",
    "        if (max(val_acc) == Validation_Accuracy):\n",
    "            saver.save(sess, chkpt)\n",
    "            print(\"Intermediate Model Save\")\n",
    "            summary_str = sess.run(summary, feed_dict=inputs)\n",
    "            summary_writer.add_summary(summary_str, i)\n",
    "            \n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,chkpt)\n",
    "    test_accuracy = evaluate(X_test, y_test, 500, False)\n",
    "    plot_confusion_matrix(X_test, y_test, 2000, False)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,chkpt)\n",
    "    plot_confusion_matrix(X_test,y_test, 2000, is_tr= False, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from skimage import exposure\n",
    "from skimage import img_as_ubyte\n",
    "from skimage import img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import importlib\n",
    "import PreProcessing\n",
    "importlib.reload(PreProcessing)\n",
    "from PreProcessing import visualize_predictions, image_normalizer, images_show\n",
    "from tensorflow.python.framework import graph_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Load Model and Open \n",
    "def run_model(X_,image_names):\n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "        total_pred,top5_pred = sess.run([prediction,top5_guesses], feed_dict={x:X_,is_training: False})\n",
    "    return total_pred, top5_pred\n",
    "\n",
    "def test_classifier(img_path):\n",
    "\n",
    "    Images = [image_normalizer(cv2.imread(img_path+name,1)) for name in os.listdir(img_path) if (name.endswith('.png')  or name.endswith('.jpg'))]\n",
    "    FileNames = [name for name in os.listdir(img_path) if ( name.endswith('.png') or name.endswith('.jpg') )]\n",
    "    Images_ = img_as_float(Images)\n",
    "    print(\"Processed Images\")\n",
    "    images_show(Images_,1,len(FileNames),rand=False)\n",
    "    total_pred, top5_pred = run_model(Images_,FileNames)\n",
    "    visualize_predictions(Images_,FileNames,total_pred, top5_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
